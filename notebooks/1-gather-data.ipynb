{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Candles – Gather Data\n",
    "----\n",
    "\n",
    "**Author:** Simon Aytes\n",
    "\n",
    "**[GitHub](https://github.com/SimonAytes)**\n",
    "\n",
    "**[Website](https://www.saytes.io)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectorlib import Extractor\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from dateutil import parser as dateparser\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to the repo directory\n",
    "dir_path = \"/\".join(os.getcwd().split(\"/\")[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configure scraping environment\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gather product page ASIN numbers from TXT file\n",
    "\n",
    "ASIN numbers (Amazon Standard Identification Number) are unique 10-character identifiers used by Amazon for product identification.\n",
    "\n",
    "In this project, we will be using a pre-gathered set of ASIN numbers that correspond to Yankee Candles listed on Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 106 ASINS\n"
     ]
    }
   ],
   "source": [
    "asins_file = open((dir_path + \"/data/asins.txt\"), \"r\")\n",
    "content = asins_file.read()\n",
    "asins = content.split(\"\\n\")\n",
    "asins_file.close()\n",
    "print(f\"Imported {len(asins)} ASINS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import extractor settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Extractor by reading from the YAML file\n",
    "extractor = Extractor.from_yaml_file(dir_path + '/src/selectors.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define utility functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main function for scraping the contents of a page\n",
    "def scrape_page(url):    \n",
    "    headers = {\n",
    "        'authority': 'www.amazon.com',\n",
    "        'pragma': 'no-cache',\n",
    "        'cache-control': 'no-cache',\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "    \n",
    "    # Download the page\n",
    "    r = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Return the HTML as text \n",
    "    return extractor.extract(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a review URL from the ASIN number\n",
    "def get_page_url(asin, page_no, limit):\n",
    "    # Define the template URL\n",
    "    t_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "    \n",
    "    # Check if the target page is after page 1\n",
    "    #if page_no > 0:\n",
    "        # Check that the target page is within the limit\n",
    "    if page_no <= limit:\n",
    "        t_url = t_url + f\"/?pageNumber={page_no+1}\"\n",
    "        return t_url\n",
    "    # If these conditions are not met, return the template URL as-is\n",
    "    elif page_no == 0:\n",
    "        return t_url\n",
    "    \n",
    "    # Else return the flag string\n",
    "    return \"FAILS CONDITION CHECK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gather reviews\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define scraping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame with column names\n",
    "reviews = pd.DataFrame(columns = [\"product_title\", \"asin\", \"author\", \"review\", \"rating\", \"date\", \"url\"])\n",
    "\n",
    "# Max. number of pages to scrape (assume 10 reviews per page)\n",
    "limit = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scrape product reviews\n",
    "\n",
    "*Note: This process may take upwards of 10 minutes to complete. When it is finished, it will print a success message.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping...\n",
      "Downloading B000WUFVR0 (1 / 106)\n",
      "Downloading B001D6HB0M (2 / 106)\n",
      "Downloading B000W3V8S8 (3 / 106)\n",
      "Downloading B007FSDIJA (4 / 106)\n",
      "Downloading B01MTS599T (5 / 106)\n",
      "Downloading B000X457HO (6 / 106)\n",
      "Downloading B000ORX6WI (7 / 106)\n",
      "Downloading B000JDGC78 (8 / 106)\n",
      "Downloading B0032JHSP6 (9 / 106)\n",
      "Downloading B078VJPW4Z (10 / 106)\n",
      "Downloading B07WDYMGZN (11 / 106)\n",
      "Downloading B004USM1A0 (12 / 106)\n",
      "Downloading B000C2TB6U (13 / 106)\n",
      "Downloading B004G9DV66 (14 / 106)\n",
      "Downloading B0057I5TIS (15 / 106)\n",
      "Downloading B001PAPPKY (16 / 106)\n",
      "Downloading B004G9C0SQ (17 / 106)\n",
      "Downloading B002UE6YQ8 (18 / 106)\n",
      "Downloading B001U40C6W (19 / 106)\n",
      "Downloading B000P6THK8 (20 / 106)\n",
      "Downloading B07PD1GJDN (21 / 106)\n",
      "Downloading B00J6CH1VO (22 / 106)\n",
      "Downloading B08TZT1N2C (23 / 106)\n",
      "Downloading B08TZ7VTNX (24 / 106)\n",
      "Downloading B07D37WXHH (25 / 106)\n",
      "Downloading B00069ZDJS (26 / 106)\n",
      "Downloading B07F82KQPZ (27 / 106)\n",
      "Downloading B07T94X7GB (28 / 106)\n",
      "Downloading B08TZRRBMZ (29 / 106)\n",
      "Downloading B07D39RV2K (30 / 106)\n",
      "Downloading B08TZS82PY (31 / 106)\n",
      "Downloading B0044R5L6S (32 / 106)\n",
      "Downloading B07D396V8Y (33 / 106)\n",
      "Downloading B07D39HSRQ (34 / 106)\n",
      "Downloading B00O60M2D8 (35 / 106)\n",
      "Downloading B01NAAQLZ1 (36 / 106)\n",
      "Downloading B06X6H95GW (37 / 106)\n",
      "Downloading B000WQZ5PC (38 / 106)\n",
      "Downloading B005OMNPII (39 / 106)\n",
      "Downloading B07P9S8VZR (40 / 106)\n",
      "Downloading B001U3WTP0 (41 / 106)\n",
      "Downloading B0099X1DFU (42 / 106)\n",
      "Downloading B07D3B3Q84 (43 / 106)\n",
      "Downloading B00O60L28E (44 / 106)\n",
      "Downloading B000TVJ6XW (45 / 106)\n",
      "Downloading B07D37ZFBH (46 / 106)\n",
      "Downloading B0044R1W98 (47 / 106)\n",
      "Downloading B000CB4UQM (48 / 106)\n",
      "Downloading B07ZQQVC9S (49 / 106)\n",
      "Downloading B019PN2DX4 (50 / 106)\n",
      "Downloading B07GV8JBZN (51 / 106)\n",
      "Downloading B004UAI0B2 (52 / 106)\n",
      "Downloading B07QK1W5BB (53 / 106)\n",
      "Downloading B0842JP2R5 (54 / 106)\n",
      "Downloading B082MQNWP5 (55 / 106)\n",
      "Downloading B081ZHJKRR (56 / 106)\n",
      "Downloading B0842JFKK6 (57 / 106)\n",
      "Downloading B076PVV9YH (58 / 106)\n",
      "Downloading B084CVW6JR (59 / 106)\n",
      "Downloading B00JJXLDGE (60 / 106)\n",
      "Downloading B07QYB6R2Q (61 / 106)\n",
      "Downloading B003U2J4XY (62 / 106)\n",
      "Downloading B083Z1ZZ6Y (63 / 106)\n",
      "Downloading B0155OG8LK (64 / 106)\n",
      "Downloading B07YZN4X7V (65 / 106)\n",
      "Downloading B07PD7ZSY9 (66 / 106)\n",
      "Downloading B0155OGEUU (67 / 106)\n",
      "Downloading B0842JLJGH (68 / 106)\n",
      "Downloading B08PMPNSS4 (69 / 106)\n",
      "Downloading B07SLT3C47 (70 / 106)\n",
      "Downloading B07D3B3Q85 (71 / 106)\n",
      "Downloading B0842JQD7B (72 / 106)\n",
      "Downloading B0842JHF9N (73 / 106)\n",
      "Downloading B07PD84GR2 (74 / 106)\n",
      "Downloading B07QGPQ5FG (75 / 106)\n",
      "Downloading B07QGQ3R73 (76 / 106)\n",
      "Downloading B0842JMGYP (77 / 106)\n",
      "Downloading B082X6RTSB (78 / 106)\n",
      "Downloading B01AL6DN1A (79 / 106)\n",
      "Downloading B08SR4PJN8 (80 / 106)\n",
      "Downloading B07PD7ZSY7 (81 / 106)\n",
      "Downloading B08SQZB27R (82 / 106)\n",
      "Downloading B07D4HH32G (83 / 106)\n",
      "Downloading B0842JN4R4 (84 / 106)\n",
      "Downloading B09C4469BH (85 / 106)\n",
      "Downloading B07Q78FNMK (86 / 106)\n",
      "Downloading B01GIDWCPS (87 / 106)\n",
      "Downloading B07T6CZYPG (88 / 106)\n",
      "Downloading B0995CPL62 (89 / 106)\n",
      "Downloading B000W3T9GG (90 / 106)\n",
      "Downloading B071KV69D3 (91 / 106)\n",
      "Downloading B06VVL5YBT (92 / 106)\n",
      "Downloading B07P9S8RJT (93 / 106)\n",
      "Downloading B07PJ1NF4J (94 / 106)\n",
      "Downloading B07D4HFPZQ (95 / 106)\n",
      "Downloading B083YSKXP5 (96 / 106)\n",
      "Downloading B0842JFM6P (97 / 106)\n",
      "Downloading B08PN6LM47 (98 / 106)\n",
      "Downloading B01N8WUHDI (99 / 106)\n",
      "Downloading B00O4O16GA (100 / 106)\n",
      "Downloading B01BKS87LY (101 / 106)\n",
      "Downloading B07WZGY3LM (102 / 106)\n",
      "Downloading B000RY54LO (103 / 106)\n",
      "Downloading B01N8SP1J3 (104 / 106)\n",
      "Downloading B07D391NSS (105 / 106)\n",
      "Downloading B07TC7X461 (106 / 106)\n",
      "Done gathering 9939 reviews in 486 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "tick = dt.datetime.now()\n",
    "\n",
    "# Print start message\n",
    "print(\"Scraping...\")\n",
    "\n",
    "# Scrape urls for reviews and append to data\n",
    "for asin in asins:\n",
    "    print(f\"Downloading {asin} ({asins.index(asin)+1} / {len(asins)})\")\n",
    "    # Reset loop variables\n",
    "    page_flag = True\n",
    "    page_no = 0\n",
    "    \n",
    "    # Loop through a product's reviews until the loop variables are flagged\n",
    "    while page_flag:\n",
    "        curr_url = get_page_url(asin, page_no, limit)\n",
    "        \n",
    "        # Check for a valid URL\n",
    "        if \"FAILS CONDITION CHECK\" in curr_url:\n",
    "            page_flag = False\n",
    "            break\n",
    "        \n",
    "        # Gather data from scraper\n",
    "        data = scrape_page(curr_url)\n",
    "        \n",
    "        # Try to parse data, set flag to false otherwise\n",
    "        try:\n",
    "            # Proceed if data was recieved\n",
    "            if data:\n",
    "                for r in data['reviews']:\n",
    "                    # Create a temporary df to store gathered data\n",
    "                    t_df = {\"product_title\":data['product_title'],\n",
    "                            \"asin\":asin,\n",
    "                            \"author\":r['author'], \n",
    "                            \"review\":r['content'], \n",
    "                            \"rating\":float(r['rating'].split(' out of')[0]), \n",
    "                            \"date\":dateparser.parse(r['date'].split('on ')[-1]).strftime('%Y-%m-%d'), \n",
    "                            \"url\":curr_url}\n",
    "\n",
    "                    # Append temporary df to reviews df\n",
    "                    reviews = reviews.append(t_df, ignore_index = True)\n",
    "            # If no data was recieved, flag output\n",
    "            else:\n",
    "                print(f\"Page flag thrown at page {page_no}\")\n",
    "                page_flag = False\n",
    "\n",
    "            # Increment page number\n",
    "            page_no = page_no + 1\n",
    "        # Catch any exceptions and continue to next ASIN no.\n",
    "        except Exception as e:\n",
    "            page_flag = False\n",
    "            #print(e)\n",
    "\n",
    "# Stop timer            \n",
    "tock = dt.datetime.now() - tick\n",
    "\n",
    "# Print success message with information\n",
    "print(f\"Done gathering {reviews.shape[0]} reviews in {tock.seconds} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(open(dir_path + \"/data/raw/reviews.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>asin</th>\n",
       "      <th>author</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yankee Candle Large Jar Candle Home Sweet Home</td>\n",
       "      <td>B000WUFVR0</td>\n",
       "      <td>David C.</td>\n",
       "      <td>I usually have good experience with Yankee Can...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>https://www.amazon.com/product-reviews/B000WUF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yankee Candle Large Jar Candle Home Sweet Home</td>\n",
       "      <td>B000WUFVR0</td>\n",
       "      <td>Vanessa</td>\n",
       "      <td>I don’t think this is a real yankee candle. Th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>https://www.amazon.com/product-reviews/B000WUF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yankee Candle Large Jar Candle Home Sweet Home</td>\n",
       "      <td>B000WUFVR0</td>\n",
       "      <td>keith e.</td>\n",
       "      <td>If you like a cinnamon scent with a hint of ap...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-08-25</td>\n",
       "      <td>https://www.amazon.com/product-reviews/B000WUF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yankee Candle Large Jar Candle Home Sweet Home</td>\n",
       "      <td>B000WUFVR0</td>\n",
       "      <td>Concerned Citizen</td>\n",
       "      <td>I read reviews of this and another version whe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>https://www.amazon.com/product-reviews/B000WUF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yankee Candle Large Jar Candle Home Sweet Home</td>\n",
       "      <td>B000WUFVR0</td>\n",
       "      <td>Wayne Connell</td>\n",
       "      <td>I purchase Yankee Candles regularly, an array ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>https://www.amazon.com/product-reviews/B000WUF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    product_title        asin  \\\n",
       "0  Yankee Candle Large Jar Candle Home Sweet Home  B000WUFVR0   \n",
       "1  Yankee Candle Large Jar Candle Home Sweet Home  B000WUFVR0   \n",
       "2  Yankee Candle Large Jar Candle Home Sweet Home  B000WUFVR0   \n",
       "3  Yankee Candle Large Jar Candle Home Sweet Home  B000WUFVR0   \n",
       "4  Yankee Candle Large Jar Candle Home Sweet Home  B000WUFVR0   \n",
       "\n",
       "              author                                             review  \\\n",
       "0           David C.  I usually have good experience with Yankee Can...   \n",
       "1            Vanessa  I don’t think this is a real yankee candle. Th...   \n",
       "2           keith e.  If you like a cinnamon scent with a hint of ap...   \n",
       "3  Concerned Citizen  I read reviews of this and another version whe...   \n",
       "4      Wayne Connell  I purchase Yankee Candles regularly, an array ...   \n",
       "\n",
       "   rating        date                                                url  \n",
       "0     2.0  2019-01-18  https://www.amazon.com/product-reviews/B000WUF...  \n",
       "1     1.0  2018-12-16  https://www.amazon.com/product-reviews/B000WUF...  \n",
       "2     5.0  2019-08-25  https://www.amazon.com/product-reviews/B000WUF...  \n",
       "3     1.0  2018-11-27  https://www.amazon.com/product-reviews/B000WUF...  \n",
       "4     1.0  2016-09-18  https://www.amazon.com/product-reviews/B000WUF...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Output raw review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output reviews data to the \"raw\" folder\n",
    "reviews.to_csv((dir_path + \"/data/raw/reviews.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-process review data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include data within the given data range\n",
    "date_filter = (reviews['date'] >= \"2021-10-01\") & (reviews['date'] <= \"2022-01-01\")\n",
    "reviews = reviews.loc[date_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculate daily review counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the number of reviews on each day and output to CSV\n",
    "reviews_by_day = reviews['date'].value_counts()\n",
    "reviews_by_day = reviews_by_day.to_frame().reset_index()\n",
    "reviews_by_day = reviews_by_day.rename(columns={\"index\":\"date\", \"date\":\"freq\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Output pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRICATED\n",
    "# Output filtered reviews data to the \"interim\" folder\n",
    "reviews.to_csv(dir_path + \"/data/interim/reviews-cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output daily counts data to the \"interim\" folder\n",
    "reviews_by_day.to_csv(dir_path + \"/data/interim/reviews_by_day.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gather COVID-19 data\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Download data\n",
    "\n",
    "The COVID-19 data for this project is sourced from Our World In Data (OWID) and is updated daily. View their documentation [here](https://ourworldindata.org/coronavirus-source-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all-time COVID-19 data from OWID GitHub\n",
    "covid_data = pd.read_csv('https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>...</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>excess_mortality_cumulative_absolute</th>\n",
       "      <th>excess_mortality_cumulative</th>\n",
       "      <th>excess_mortality</th>\n",
       "      <th>excess_mortality_cumulative_per_million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso_code continent     location        date  total_cases  new_cases  \\\n",
       "0      AFG      Asia  Afghanistan  2020-02-24          5.0        5.0   \n",
       "\n",
       "   new_cases_smoothed  total_deaths  new_deaths  new_deaths_smoothed  ...  \\\n",
       "0                 NaN           NaN         NaN                  NaN  ...   \n",
       "\n",
       "   female_smokers  male_smokers  handwashing_facilities  \\\n",
       "0             NaN           NaN                  37.746   \n",
       "\n",
       "   hospital_beds_per_thousand  life_expectancy  human_development_index  \\\n",
       "0                         0.5            64.83                    0.511   \n",
       "\n",
       "   excess_mortality_cumulative_absolute  excess_mortality_cumulative  \\\n",
       "0                                   NaN                          NaN   \n",
       "\n",
       "   excess_mortality  excess_mortality_cumulative_per_million  \n",
       "0               NaN                                      NaN  \n",
       "\n",
       "[1 rows x 67 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data\n",
    "covid_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Output raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output raw data to csv in the \"raw\" folder\n",
    "covid_data.to_csv(dir_path + \"/data/raw/owid_covid_19_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-process COVID-19 data\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Select relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce columns to only those we need\n",
    "covid_data = covid_data[['date', 'iso_code', 'location', 'total_cases', 'new_cases']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Filter data by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include data from the USA\n",
    "covid_data = covid_data[covid_data['iso_code'] == \"USA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>location</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148428</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date iso_code       location  total_cases  new_cases\n",
       "148428  2020-01-22      USA  United States          1.0        NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Data\n",
    "covid_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include data within the given data range\n",
    "date_filter = (covid_data['date'] >= \"2021-10-01\") & (covid_data['date'] <= \"2022-01-16\")\n",
    "covid_data = covid_data.loc[date_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Output pre-processed COVID-19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output reviews df as a CSV\n",
    "covid_data.to_csv(dir_path + \"/data/interim/covid_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
